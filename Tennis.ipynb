{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # (Image References)\n",
    "\n",
    "[image1]: https://user-images.githubusercontent.com/10624937/42135623-e770e354-7d12-11e8-998d-29fc74429ca2.gif \"Trained Agent\"\n",
    "\n",
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will explore DDPG and self-play to learn how to play tennis using the Unity ML-Agents environment. This was part of the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "![Trained Agent][image1]\n",
    "\n",
    "## 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "from ddpg_agent import SADDPG, MiADDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approach\n",
    "\n",
    "In the previous [project](https://github.com/escorciav/drlnd-continuous-control), we saw the DDPG algorithm in practice achieving reaching a moving target (yields high-rewards) while learning continous actions from observations. Interestingly, we employed multiple _independent_ agents collecting a diverse set of experiences. This time, the _Tennis_ environment involves placing two agents interacting with each other as they learn to master the game.\n",
    "\n",
    "_Shall we use our previous approach?_\n",
    "\n",
    "That's certainly the most straighforward approach. We can place a single agent to learn the game while playing againts itself i.e. learning through self-play. In other words, we could use the same actor and critic parameters $\\theta_\\mu, \\theta_Q$ for each agent and let them interact (collect experiences) as if they were two different entities.\n",
    "\n",
    "_Doesn't sound truly multi-agent?_\n",
    "\n",
    "That's right. Given that we have a unique set of parameters $\\theta_\\mu, \\theta_Q$, this is not a truly multi-agent solution. Nevertheless, there is an additional challenge with respect to other environments, the agent needs to learn as it interacts with itself.\n",
    "\n",
    "### 3.1 Actor-Critic and hyper-parameters details\n",
    "\n",
    "For simpliciy and modularity, all the inner details of our actor and critic networks are in this [module](https://github.com/escorciav/drlnd-continuous-control/blob/master/model.py). We used the same hyper-parameter configuration described in the [DDPG paper](https://arxiv.org/pdf/1509.02971.pdf).\n",
    "\n",
    "For the _Actor_, we use a MLP with a single hidden layer. In practice, we project the state space into a space of 400 dimension through a Linear layer. The size of our hidden layer is 300 dimensions. Finally, our output layer projects the output of our hidden layer into the size of the action space. With the exception of the last linear projection layer, all the others are followed by a ReLU non-linearity. The last linear layer is followed by a `tanh` non-linearity ensuring that the output range fits between $[-1, 1]$ i.e. the admisible values of our actuators.\n",
    "\n",
    "The _Critic_ network is similar to the _Actor_. The only notable differences are:\n",
    "\n",
    "- The size of our output layer is reduced to 1 and we remove any non-lineary after it, as it is estimating the value the combined state and action pair.\n",
    "    \n",
    "- We fuses the state and action in the \"hidden layer\" of our network as presented in the DDPG paper.\n",
    "    \n",
    "The following figure depicts the details of our Actor adn Critic networks.\n",
    "\n",
    "![Architecture](https://github.com/escorciav/drlnd-continuous-control/raw/master/data/diagram.png)\n",
    "\n",
    "_Optimization and exploration and others details_\n",
    "\n",
    "We update the target networks after each learning steps with a $\\tau$ value equal to 1e-3 i.e. after each learning update our target actor-critic networks move closer to the new actor-critics networks by 0.1%. The size of our buffer is 100k experiences and the batch size is 128. The discount factor for future rewards is set 0.99.\n",
    "\n",
    "As epxloration strategy, we use noise in the action space. In particular, the noise is sampled from [Ornstein-Uhlenbeck process](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) with $\\theta=0.15$, $\\sigma=0.2$. In this way, we can add temporally-correlated noise which is of particular interest for continous problems such us our Reacher.\n",
    "\n",
    "Here, we use the ADAM optimizer learning rates of 1e-3 and 1e-4 for the critic and the actor, respectively. The target actor-critic networks are updated via soft-updates using $\\tau=1e-3$\n",
    "\n",
    "### 3.2 Time of (Agent) learning\n",
    "\n",
    "Now it's time to train our own agent to solve the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 350/3500 [01:43<15:41,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [350] Max score  0.0000 Avg score 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 700/3500 [04:20<1:06:19,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [700] Max score  0.1900 Avg score 0.0854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1051/3500 [10:14<23:29,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [1050] Max score  0.0000 Avg score 0.1363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1400/3500 [19:48<1:34:28,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [1400] Max score  0.1000 Avg score 0.2560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1750/3500 [31:04<34:47,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [1750] Max score  0.1000 Avg score 0.2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 2100/3500 [45:41<38:43,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode [2100] Max score  0.2000 Avg score 0.3873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 2130/3500 [48:19<2:24:35,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Challenge solved after 2131 episodes\n"
     ]
    }
   ],
   "source": [
    "random_seed = 1701\n",
    "num_episodes = 3500\n",
    "is_solved_score = 0.5\n",
    "print_freq = 0.1\n",
    "\n",
    "#############################################################\n",
    "# Agent(s) setup\n",
    "agent = SADDPG(\n",
    "    state_size=state_size, action_size=action_size,\n",
    "    random_seed=random_seed)\n",
    "#############################################################\n",
    "\n",
    "best_score = -np.inf\n",
    "scores_window = deque(maxlen=100)\n",
    "score_episodes, mov_avg_score_episodes = [], []\n",
    "if print_freq < 1:\n",
    "    print_freq = max(int(print_freq * num_episodes), 1)\n",
    "\n",
    "for i_episode in tqdm(range(1, num_episodes + 1)):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    \n",
    "    while True:\n",
    "        actions = agent.act(states)\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        agent.step(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if any(dones):\n",
    "            break\n",
    "    \n",
    "    max_score = scores.max()\n",
    "    scores_window.append(max_score)\n",
    "    score_episodes.append(max_score)\n",
    "    mov_avg_score_episodes.append(np.mean(scores_window))\n",
    "    \n",
    "    if i_episode % print_freq == 0:\n",
    "        print(f'Episode [{i_episode}] Max score  {max_score:.4f} '\n",
    "              f'Avg score {mov_avg_score_episodes[-1]:.4f}')\n",
    "    \n",
    "    if max_score > best_score:\n",
    "        agent.save()\n",
    "        best_score = max_score\n",
    "    \n",
    "    if mov_avg_score_episodes[-1] >= is_solved_score:\n",
    "        print(f'Challenge solved after {i_episode:d} episodes')\n",
    "        agent.save()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the progression of our agent along the course of all the episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episodes = range(1, len(score_episodes) + 1)\n",
    "plt.plot(episodes, score_episodes, lw=3)\n",
    "plt.plot(episodes, mov_avg_score_episodes, lw=2, color='orange')\n",
    "plt.xlabel('Episodes', fontsize=14)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "_ = plt.yticks(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, hopefully you also managed to train the \"agents\" without any inconvenient. The progress was slowly and a bit noisy, but it managed to reach the milestone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Watch a pair of smart agents\n",
    "\n",
    "It's time to watch the performance of our autonomous agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (max over agents) from episode 1: 0.0\n",
      "Score (max over agents) from episode 2: 0.10000000149011612\n",
      "Score (max over agents) from episode 3: 0.10000000149011612\n",
      "Score (max over agents) from episode 4: 0.10000000149011612\n",
      "Score (max over agents) from episode 5: 1.1000000163912773\n"
     ]
    }
   ],
   "source": [
    "#############################################################\n",
    "# Agent(s) setup\n",
    "random_seed = 1701\n",
    "agent = SADDPG(\n",
    "    state_size=state_size, action_size=action_size,\n",
    "    random_seed=random_seed)\n",
    "agent.load('checkpoint.pth')\n",
    "#############################################################\n",
    "\n",
    "for i in range(1, 6):                                      # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    while True:\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones = env_info.local_done\n",
    "        scores += env_info.rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):\n",
    "            break\n",
    "    print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Further explorations\n",
    "\n",
    "The previous solution corresponds to the 1st iteration to solve the _Tennis_ challenge. The following picture summarizes the learning journey afterwards:\n",
    "\n",
    "![Comparison](https://github.com/escorciav/drlnd-collab-compet/raw/master/data/plot_comparison.png)\n",
    "\n",
    "The top-left corner corresponds to the previous solution where a single agent that learns to master the game through self-play. Naturally, the subsequent question is:\n",
    "\n",
    "_Can we do better?_\n",
    "\n",
    "The first change that we pursued came from the fact that the learning progress was too slow, and a bit shaky. Around the episode 1500, the agent was progressively learning until its average score suddenly decreases. This suggested that we could tune the rate of change of our network parameters ($\\theta_\\mu,\\theta_Q$). Therefore, we increased the  learning rate (1e-2, 1e-3, 1e-1) for our actor-critic networks. However, this did not work at all. On the other hand, we observed that increasing the update rate of our target network ($\\tau$) was extremely benefitial. This simple change allows to reduce the number of episodes required to solve the challenge by **4.3x**. Th evidence of that is depicted in *Figure (b)*.\n",
    "\n",
    "After exploring the hyper-parameter space a bit, we feel that it was time to consider architectural changes to our solution. In particular,\n",
    "\n",
    "_Does the addition of multiple agents reduces the numbers of episodes required to solve the challenge?_\n",
    "\n",
    "The most straigforward solution is to train two independent agents i.e. two pairs of actor-critic networks with different $\\theta_\\mu,\\theta_Q$ parameters.\n",
    "Interestingly, this approach works out of the shelf and reduces the number of episodes wrt our initial solution. However, it takes roughly **2.8x** more episodes to train than our fastest solution.\n",
    "Please note that the agents shared the replay buffer as it may introduce more diversity on the mini-batches.\n",
    "\n",
    "Finally, in the context of multiple agents and actor-critic methods we can consider more interesting variants. For example, sharing the _Critic_ network among different _Actors_. This could be analogous hire the same couch to shape our agents. *Figure (d)* shows the results of this approach. First, it is interesting to note that sharing the _Critic_ network yields a reduction of **2.4x** in the number of episodes to reach the milestone in comparison to two independent _Critics_ (_Figure (c)_ v.s. _Figure (d)_). Overall, the reduction is competitive to our fastest approach, _Figure (b)_, and we observe that the number of episodes may be similar for other seeds.\n",
    "\n",
    "In case you want to try one of the following solution, feel free to edit the cells above replacing the agent setup enclosed inside the comments with one of the following configurations.\n",
    "\n",
    "```python\n",
    "# Fig (b)\n",
    "# Single-agent\n",
    "# The fastest trainig\n",
    "agent = SADDPG(\n",
    "    state_size=state_size, action_size=action_size,\n",
    "    random_seed=random_seed, tau=1e-2)\n",
    "\n",
    "# Fig (c)\n",
    "# Multi-agent (independent actors and critic)\n",
    "# Train slower than Fig-b, but faster than previous solution\n",
    "agent = MiADDPG(\n",
    "    num_agents=num_agents, state_size=state_size,\n",
    "    action_size=action_size, random_seed=random_seed,\n",
    "    tau=1e-2, share_critic=False)\n",
    "\n",
    "# Fig (d)\n",
    "# Multi-agent (independent actors, same critic/couch)\n",
    "# Competitive speed wrt to Fig (b)\n",
    "agent = MiADDPG(\n",
    "    num_agents=num_agents, state_size=state_size,\n",
    "    action_size=action_size, random_seed=random_seed,\n",
    "    tau=1e-2, share_critic=True)\n",
    "```\n",
    "\n",
    "You can find checkpoints of all these agents [here](https://github.com/escorciav/drlnd-collab-compet/raw/master/checkpoints/).\n",
    "\n",
    "Feel free to dig into the training details of the `SADDPG` and `MiADDPG` abstractions to train a single and multiple agents, respectively. Please share suggestions about how to refactor the code to make it more readable or maintainable.\n",
    "\n",
    "_Implementation notes_\n",
    "\n",
    "- Computational complexity and sampling budget of single and multi-agent approaches\n",
    "\n",
    "  It is unfair and misleading to compare both approaches without disclosing the computational and sampling scheme details of them.\n",
    "  In our case, the networks were  effectively trained with the same number of examples per mini-batch. Thus, a reduction in the number of episodes to reach the milestone translates into an efficient use of training examples.\n",
    "  Moreover, all our schemes have an equivalent computational complexity during inference regardless of the implementation details.\n",
    "  During training, the computational complexity of our multi-agent approach scales linearly with the number of agents and the computational complexity of training a single agent. In other words, if we spend 1s doing a forward-backward pass training a single agent, we would expect to spend 2s for our multi-agent solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Departing words\n",
    "\n",
    "The main take away from this project is that agents can learn through self-play. Interestingly, we did not require adjustments to the standard DDPG algorithm to solve the _Tennis_ challenge. There are more steps to improve the learning of our autonomous agents. For example:\n",
    "\n",
    "- Prioritized experience replay. Currently, the past experiences used to train the brain is by sampling uniform random. However, we could argue that some experiences may be more \"important\" than others. Moreover, it is possible that such \"important\" experiences are infrequent and our agents ends up sampling them less often. Therefore, it would be ideal to implement a mechanism to sampling experiences in a non-uniform way based on some criterion.\n",
    "\n",
    "- Noise in the parameteres space. To account for more exploration and possibly more robustness to unseen conditions and states, it would be important to implemente the noise in the parameters strategy dicussed [here](https://blog.openai.com/better-exploration-with-parameter-noise/).\n",
    "\n",
    "- Multi-agent approaches. It would be interesting to observe if more advanced multi-agent algorithms such as [MADDPG](https://arxiv.org/pdf/1706.02275.pdf) reduces the number of episodes required to achieve the same milestone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
